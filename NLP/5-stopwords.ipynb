{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stopwords = are the words that are filtered out before or after processing of text.\n",
    "# They are common words that do not carry significant meaning in a sentence, such as \"the\", \"is\", \"in\", \"and\", etc.\n",
    "# Stopwords are often removed from text data to reduce noise and improve the performance of natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0d70f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Panks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe35b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## why stopwords are removed?\n",
    "# 1. to reduce size of the text data\n",
    "# 2. to reduce complexity of the text data\n",
    "# 3. to reduce dimensionality of the text data\n",
    "# 4. to reduce noise in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b33b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ae355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d18246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Stopwords are the most common words in a language like 'is', 'and', 'the', which do not add much meaning to a sentence.\n",
    "They are usually removed during the preprocessing stage of natural language processing tasks.\n",
    "For example, in the sentence 'The food was very good and the service was excellent', the important words are 'food', 'good', 'service', and 'excellent'.\n",
    "Removing stopwords helps in focusing on meaningful words that carry actual sentiment or context.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9df79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nStopwords are the most common words in a language like 'is', 'and', 'the', which do not add much meaning to a sentence.\",\n",
       " 'They are usually removed during the preprocessing stage of natural language processing tasks.',\n",
       " \"For example, in the sentence 'The food was very good and the service was excellent', the important words are 'food', 'good', 'service', and 'excellent'.\",\n",
       " 'Removing stopwords helps in focusing on meaningful words that carry actual sentiment or context.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844d382",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sent)):\n\u001b[32m      2\u001b[39m     words = nltk.word_tokenize(sent[i])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     words = \u001b[43m[\u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m     sent[i] = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(words)\n\u001b[32m      5\u001b[39m sent\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sent)):\n\u001b[32m      2\u001b[39m     words = nltk.word_tokenize(sent[i])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     words = [\u001b[43mstemmer\u001b[49m.stem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))]\n\u001b[32m      4\u001b[39m     sent[i] = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(words)\n\u001b[32m      5\u001b[39m sent\n",
      "\u001b[31mNameError\u001b[39m: name 'stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent)):\n",
    "    words = nltk.word_tokenize(sent[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6e78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1d70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
